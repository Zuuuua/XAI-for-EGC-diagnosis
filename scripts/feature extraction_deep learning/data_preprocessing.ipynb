{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35bae71a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T03:33:33.372550Z",
     "start_time": "2023-03-06T03:33:32.943805Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, shutil, random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d31455",
   "metadata": {},
   "source": [
    "## Clear_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de548acc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:22:12.870460Z",
     "start_time": "2023-06-13T01:22:12.860487Z"
    }
   },
   "outputs": [],
   "source": [
    "imgs_dir = r\"~\\data\\imgs_feature_extraction_deep_learning\\imgs_boundary_clear\\train\"\n",
    "n_labels = len(os.listdir(imgs_dir))\n",
    "\n",
    "label_dir = os.path.split(imgs_dir)[0]+'\\\\labels'\n",
    "if not os.path.exists(label_dir):\n",
    "    os.makedirs(label_dir)\n",
    "    \n",
    "imgs_0_dir = os.path.join(imgs_dir, '0')\n",
    "imgs_1_dir = os.path.join(imgs_dir, '1')\n",
    "\n",
    "file_0_all = os.listdir(imgs_0_dir)\n",
    "file_1_all = os.listdir(imgs_1_dir)\n",
    "\n",
    "random.shuffle(file_1_all)\n",
    "random.shuffle(file_0_all)\n",
    "\n",
    "if n_labels == 3:\n",
    "    imgs_2_dir = os.path.join(imgs_dir, '2')\n",
    "    file_2_all = os.listdir(imgs_2_dir)\n",
    "    random.shuffle(file_2_all)\n",
    "    \n",
    "print(len(file_0_all),len(file_1_all),len(file_2_all)) if n_labels == 3  else print(len(file_0_all),len(file_1_all))\n",
    "\n",
    "for i in range(1, 11):\n",
    "    rate = i * 0.1\n",
    "    file_0_list = file_0_all[:int(len(file_0_all)*rate)]\n",
    "    file_1_list = file_1_all[:int(len(file_1_all)*rate)]\n",
    "\n",
    "    for j in range(len(file_0_list)):\n",
    "        file_0_list[j] = imgs_dir + '\\\\0\\\\' + file_0_list[j] + ',0'\n",
    "    for k in range(len(file_1_list)):\n",
    "        file_1_list[k] = imgs_dir + '\\\\1\\\\' + file_1_list[k] + ',1'\n",
    "    \n",
    "    label_list = file_0_list[:int(len(file_0_list)*0.9)] + file_1_list[:int(len(file_1_list)*0.9)]\n",
    "    val_list = file_0_list[int(len(file_0_list)*0.9):] + file_1_list[int(len(file_1_list)*0.9):]\n",
    "    \n",
    "    if n_labels == 3:\n",
    "        file_2_list = file_2_all[:int(len(file_2_all)*rate)]\n",
    "        \n",
    "        for q in range(len(file_2_list)):\n",
    "            file_2_list[q] = imgs_dir + '\\\\2\\\\' + file_2_list[q] + ',2'\n",
    "            \n",
    "        label_list += file_2_list[:int(len(file_2_list)*0.9)]\n",
    "        val_list += file_2_list[int(len(file_2_list)*0.9):]\n",
    "    \n",
    "    if n_labels == 3:\n",
    "        print(len(label_list), len(val_list), len(file_0_list), len(file_1_list), len(file_2_list)) \n",
    "    else:\n",
    "        print(len(label_list), len(val_list), len(file_0_list), len(file_1_list))\n",
    "    \n",
    "    \n",
    "    label_txt_path = os.path.join(label_dir, 'label_'+str(i*10)+'.txt')\n",
    "    val_label_txt_path = os.path.join(label_dir, 'val_label_'+str(i*10)+'.txt')\n",
    "\n",
    "    with open(label_txt_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for l in range(len(label_list)):\n",
    "            f.write(label_list[l]+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "    with open(val_label_txt_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for m in range(len(val_list)):\n",
    "            f.write(val_list[m]+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "for i in range(1, 11):\n",
    "    train_list = label_list+val_list\n",
    "\n",
    "    val_label_txt_path = os.path.join(label_dir, 'val_label_'+str(i*10)+'.txt')\n",
    "    train_txt_path = os.path.join(label_dir, 'train_'+str(i*10)+'.txt')\n",
    "    \n",
    "    f_val = open(val_label_txt_path, 'r', encoding='utf-8')\n",
    "    val_file_list = f_val.readlines()\n",
    "    for val_file in val_file_list:\n",
    "        if val_file[:-1] in train_list:\n",
    "\n",
    "            train_list.remove(val_file[:-1])\n",
    "\n",
    "    with open(train_txt_path, 'w', encoding=\"utf-8\") as f_train:\n",
    "        for n in range(len(train_list)):\n",
    "            f_train.write(train_list[n]+'\\n')\n",
    "        f_train.close()\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "723f7e2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:23:15.189438Z",
     "start_time": "2023-06-13T01:23:15.172484Z"
    }
   },
   "outputs": [],
   "source": [
    "# randomly select 400 images from each group, calculate the mean and std\n",
    "\n",
    "root = os.path.split(os.path.split(imgs_dir)[0])[0]\n",
    "train_root = root + r'\\\\imgs_boundary_clear\\train'\n",
    "val_path = root + r'\\imgs_boundary_clear'\n",
    "subs = os.listdir(train_root)\n",
    "min_imgs = 10000000\n",
    "subs = os.listdir(train_root)\n",
    "for sub in subs:\n",
    "    abs_sub = os.path.join(train_root, sub)\n",
    "    files = os.listdir(abs_sub)\n",
    "    count = len(files)\n",
    "    print(sub, ':', count)\n",
    "    if count < min_imgs:\n",
    "        min_imgs = count       \n",
    "img_count = min(400, min_imgs)\n",
    "print('to chk :', img_count)\n",
    "\n",
    "img_h,img_w = 224,224\n",
    "imgs = np.zeros([img_w,img_h,3,1])\n",
    "means,stdevs = [],[]\n",
    "\n",
    "subs = os.listdir(train_root)\n",
    "for sub in subs:\n",
    "    abs_sub = os.path.join(train_root, sub)\n",
    "    files = os.listdir(abs_sub)\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    for i in tqdm_notebook(range(img_count)):\n",
    "        img = cv2.imdecode(np.fromfile(os.path.join(abs_sub, files[i]), dtype=np.uint8), -1)\n",
    "        img = cv2.resize(img,(img_h,img_w))\n",
    "        img = img[:,:,:,np.newaxis]\n",
    "\n",
    "        imgs = np.concatenate((imgs,img),axis=3)\n",
    "              \n",
    "imgs = imgs[:,:,:,1:]\n",
    "print(imgs.shape)\n",
    "imgs = imgs.astype(np.float32)/255.\n",
    " \n",
    "for i in tqdm_notebook(range(3)):\n",
    "    pixels = imgs[:,i,:].ravel() # pull in a row\n",
    "    means.append(round(np.mean(pixels), 4))\n",
    "    stdevs.append(round(np.std(pixels), 4))\n",
    " \n",
    "# The image format read by cv2 is BGR, and the image read by PIL/Skimage is RGB without need for conversion\n",
    "means.reverse() # BGR --> RGB\n",
    "stdevs.reverse()\n",
    " \n",
    "print(\"normMean = {}\".format(means))\n",
    "print(\"normStd = {}\".format(stdevs))\n",
    "print('transforms.Normalize(normMean = {},normStd = {})'.format(means,stdevs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f78b5",
   "metadata": {},
   "source": [
    "## Surface_rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdd3c34f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:26:34.255710Z",
     "start_time": "2023-06-13T01:26:34.236760Z"
    }
   },
   "outputs": [],
   "source": [
    "imgs_dir = r\"~\\data\\imgs_feature_extraction_deep_learning\\imgs_surface_rough\\train\"\n",
    "n_labels = len(os.listdir(imgs_dir))\n",
    "\n",
    "\n",
    "label_dir = os.path.split(imgs_dir)[0]+'\\\\labels'\n",
    "if not os.path.exists(label_dir):\n",
    "    os.makedirs(label_dir)\n",
    "\n",
    "\n",
    "imgs_0_dir = os.path.join(imgs_dir, '0')\n",
    "imgs_1_dir = os.path.join(imgs_dir, '1')\n",
    "\n",
    "file_0_all = os.listdir(imgs_0_dir)\n",
    "file_1_all = os.listdir(imgs_1_dir)\n",
    "\n",
    "random.shuffle(file_1_all)\n",
    "random.shuffle(file_0_all)\n",
    "\n",
    "if n_labels == 3:\n",
    "    imgs_2_dir = os.path.join(imgs_dir, '2')\n",
    "    file_2_all = os.listdir(imgs_2_dir)\n",
    "    random.shuffle(file_2_all)\n",
    "    \n",
    "\n",
    "print(len(file_0_all),len(file_1_all),len(file_2_all)) if n_labels == 3  else print(len(file_0_all),len(file_1_all))\n",
    "\n",
    "for i in range(1, 11):\n",
    "    rate = i * 0.1\n",
    "    file_0_list = file_0_all[:int(len(file_0_all)*rate)]\n",
    "    file_1_list = file_1_all[:int(len(file_1_all)*rate)]\n",
    "\n",
    "    for j in range(len(file_0_list)):\n",
    "        file_0_list[j] = imgs_dir + '\\\\0\\\\' + file_0_list[j] + ',0'\n",
    "    for k in range(len(file_1_list)):\n",
    "        file_1_list[k] = imgs_dir + '\\\\1\\\\' + file_1_list[k] + ',1'\n",
    "    \n",
    "    label_list = file_0_list[:int(len(file_0_list)*0.9)] + file_1_list[:int(len(file_1_list)*0.9)]\n",
    "    val_list = file_0_list[int(len(file_0_list)*0.9):] + file_1_list[int(len(file_1_list)*0.9):]\n",
    "    \n",
    "    if n_labels == 3:\n",
    "        file_2_list = file_2_all[:int(len(file_2_all)*rate)]\n",
    "        \n",
    "        for q in range(len(file_2_list)):\n",
    "            file_2_list[q] = imgs_dir + '\\\\2\\\\' + file_2_list[q] + ',2'\n",
    "            \n",
    "        label_list += file_2_list[:int(len(file_2_list)*0.9)]\n",
    "        val_list += file_2_list[int(len(file_2_list)*0.9):]\n",
    "    \n",
    "    if n_labels == 3:\n",
    "        print(len(label_list), len(val_list), len(file_0_list), len(file_1_list), len(file_2_list)) \n",
    "    else:\n",
    "        print(len(label_list), len(val_list), len(file_0_list), len(file_1_list))\n",
    "    \n",
    "    label_txt_path = os.path.join(label_dir, 'label_'+str(i*10)+'.txt')\n",
    "    val_label_txt_path = os.path.join(label_dir, 'val_label_'+str(i*10)+'.txt')\n",
    "\n",
    "    with open(label_txt_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for l in range(len(label_list)):\n",
    "            f.write(label_list[l]+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "    with open(val_label_txt_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for m in range(len(val_list)):\n",
    "            f.write(val_list[m]+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "for i in range(1, 11):\n",
    "    train_list = label_list+val_list\n",
    "\n",
    "    val_label_txt_path = os.path.join(label_dir, 'val_label_'+str(i*10)+'.txt')\n",
    "    train_txt_path = os.path.join(label_dir, 'train_'+str(i*10)+'.txt')\n",
    "    \n",
    "    f_val = open(val_label_txt_path, 'r', encoding='utf-8')\n",
    "    val_file_list = f_val.readlines()\n",
    "    for val_file in val_file_list:\n",
    "        if val_file[:-1] in train_list:\n",
    "\n",
    "            train_list.remove(val_file[:-1])\n",
    "\n",
    "    with open(train_txt_path, 'w', encoding=\"utf-8\") as f_train:\n",
    "        for n in range(len(train_list)):\n",
    "            f_train.write(train_list[n]+'\\n')\n",
    "        f_train.close()\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1054321",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:26:41.377309Z",
     "start_time": "2023-06-13T01:26:41.365342Z"
    }
   },
   "outputs": [],
   "source": [
    "# randomly select 400 images from each group, calculate the mean and std\n",
    "\n",
    "root = os.path.split(os.path.split(imgs_dir)[0])[0]\n",
    "train_root = root + r'\\imgs_surface_rough\\train'\n",
    "val_path = root + r'\\imgs_surface_rough'\n",
    "subs = os.listdir(train_root)\n",
    "min_imgs = 10000000\n",
    "subs = os.listdir(train_root)\n",
    "for sub in subs:\n",
    "    abs_sub = os.path.join(train_root, sub)\n",
    "    files = os.listdir(abs_sub)\n",
    "    count = len(files)\n",
    "    print(sub, ':', count)\n",
    "    if count < min_imgs:\n",
    "        min_imgs = count       \n",
    "img_count = min(400, min_imgs)\n",
    "print('to chk :', img_count)\n",
    "img_h,img_w = 224,224\n",
    "imgs = np.zeros([img_w,img_h,3,1])\n",
    "means,stdevs = [],[]\n",
    "\n",
    "subs = os.listdir(train_root)\n",
    "for sub in subs:\n",
    "    abs_sub = os.path.join(train_root, sub)\n",
    "    files = os.listdir(abs_sub)\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    for i in tqdm_notebook(range(img_count)):\n",
    "        img = cv2.imdecode(np.fromfile(os.path.join(abs_sub, files[i]), dtype=np.uint8), -1)\n",
    "        img = cv2.resize(img,(img_h,img_w))\n",
    "        img = img[:,:,:,np.newaxis]\n",
    "\n",
    "        imgs = np.concatenate((imgs,img),axis=3)\n",
    "        \n",
    "        \n",
    "imgs = imgs[:,:,:,1:]\n",
    "print(imgs.shape)\n",
    "imgs = imgs.astype(np.float32)/255.\n",
    " \n",
    "for i in tqdm_notebook(range(3)):\n",
    "    pixels = imgs[:,i,:].ravel() # pull in a row\n",
    "    means.append(round(np.mean(pixels), 4))\n",
    "    stdevs.append(round(np.std(pixels), 4))\n",
    " \n",
    "means.reverse() # BGR --> RGB\n",
    "stdevs.reverse()\n",
    " \n",
    "print(\"normMean = {}\".format(means))\n",
    "print(\"normStd = {}\".format(stdevs))\n",
    "print('transforms.Normalize(normMean = {},normStd = {})'.format(means,stdevs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4964dc",
   "metadata": {},
   "source": [
    "## Bleeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feca63e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:27:13.022450Z",
     "start_time": "2023-06-13T01:27:13.011480Z"
    }
   },
   "outputs": [],
   "source": [
    "imgs_dir = r\"~\\data\\imgs_feature_extraction_deep_learning\\imgs_bleeding\\train\"\n",
    "n_labels = len(os.listdir(imgs_dir))\n",
    "\n",
    "label_dir = os.path.split(imgs_dir)[0]+'\\\\labels'\n",
    "if not os.path.exists(label_dir):\n",
    "    os.makedirs(label_dir)\n",
    "\n",
    "imgs_0_dir = os.path.join(imgs_dir, '0')\n",
    "imgs_1_dir = os.path.join(imgs_dir, '1')\n",
    "\n",
    "file_0_all = os.listdir(imgs_0_dir)\n",
    "file_1_all = os.listdir(imgs_1_dir)\n",
    "\n",
    "random.shuffle(file_1_all)\n",
    "random.shuffle(file_0_all)\n",
    "\n",
    "\n",
    "if n_labels == 3:\n",
    "    imgs_2_dir = os.path.join(imgs_dir, '2')\n",
    "    file_2_all = os.listdir(imgs_2_dir)\n",
    "    random.shuffle(file_2_all)\n",
    "    \n",
    "\n",
    "print(len(file_0_all),len(file_1_all),len(file_2_all)) if n_labels == 3  else print(len(file_0_all),len(file_1_all))\n",
    "\n",
    "for i in range(1, 11):\n",
    "    rate = i * 0.1\n",
    "    file_0_list = file_0_all[:int(len(file_0_all)*rate)]\n",
    "    file_1_list = file_1_all[:int(len(file_1_all)*rate)]\n",
    "\n",
    "    for j in range(len(file_0_list)):\n",
    "        file_0_list[j] = imgs_dir + '\\\\0\\\\' + file_0_list[j] + ',0'\n",
    "    for k in range(len(file_1_list)):\n",
    "        file_1_list[k] = imgs_dir + '\\\\1\\\\' + file_1_list[k] + ',1'\n",
    "    \n",
    "    label_list = file_0_list[:int(len(file_0_list)*0.9)] + file_1_list[:int(len(file_1_list)*0.9)]\n",
    "    val_list = file_0_list[int(len(file_0_list)*0.9):] + file_1_list[int(len(file_1_list)*0.9):]\n",
    "    \n",
    "    if n_labels == 3:\n",
    "        file_2_list = file_2_all[:int(len(file_2_all)*rate)]\n",
    "        \n",
    "        for q in range(len(file_2_list)):\n",
    "            file_2_list[q] = imgs_dir + '\\\\2\\\\' + file_2_list[q] + ',2'\n",
    "            \n",
    "        label_list += file_2_list[:int(len(file_2_list)*0.9)]\n",
    "        val_list += file_2_list[int(len(file_2_list)*0.9):]\n",
    "    \n",
    "    if n_labels == 3:\n",
    "        print(len(label_list), len(val_list), len(file_0_list), len(file_1_list), len(file_2_list)) \n",
    "    else:\n",
    "        print(len(label_list), len(val_list), len(file_0_list), len(file_1_list))\n",
    "    \n",
    "    \n",
    "    label_txt_path = os.path.join(label_dir, 'label_'+str(i*10)+'.txt')\n",
    "    val_label_txt_path = os.path.join(label_dir, 'val_label_'+str(i*10)+'.txt')\n",
    "\n",
    "    with open(label_txt_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for l in range(len(label_list)):\n",
    "            f.write(label_list[l]+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "    with open(val_label_txt_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for m in range(len(val_list)):\n",
    "            f.write(val_list[m]+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "for i in range(1, 11):\n",
    "    train_list = label_list+val_list\n",
    "\n",
    "    val_label_txt_path = os.path.join(label_dir, 'val_label_'+str(i*10)+'.txt')\n",
    "    train_txt_path = os.path.join(label_dir, 'train_'+str(i*10)+'.txt')\n",
    "    \n",
    "    f_val = open(val_label_txt_path, 'r', encoding='utf-8')\n",
    "    val_file_list = f_val.readlines()\n",
    "    for val_file in val_file_list:\n",
    "        if val_file[:-1] in train_list:\n",
    "\n",
    "            train_list.remove(val_file[:-1])\n",
    "\n",
    "    with open(train_txt_path, 'w', encoding=\"utf-8\") as f_train:\n",
    "        for n in range(len(train_list)):\n",
    "            f_train.write(train_list[n]+'\\n')\n",
    "        f_train.close()\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "825ef5fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:27:18.868174Z",
     "start_time": "2023-06-13T01:27:18.859198Z"
    }
   },
   "outputs": [],
   "source": [
    "# randomly select 400 images from each group, calculate the mean and std\n",
    "\n",
    "root = os.path.split(os.path.split(imgs_dir)[0])[0]\n",
    "train_root = root + r'\\imgs_bleeding\\train'\n",
    "val_path = root + r'\\imgs_bleeding'\n",
    "subs = os.listdir(train_root)\n",
    "min_imgs = 10000000\n",
    "subs = os.listdir(train_root)\n",
    "for sub in subs:\n",
    "    abs_sub = os.path.join(train_root, sub)\n",
    "    files = os.listdir(abs_sub)\n",
    "    count = len(files)\n",
    "    print(sub, ':', count)\n",
    "    if count < min_imgs:\n",
    "        min_imgs = count       \n",
    "img_count = min(400, min_imgs)\n",
    "print('to chk :', img_count)\n",
    "img_h,img_w = 224,224\n",
    "imgs = np.zeros([img_w,img_h,3,1])\n",
    "means,stdevs = [],[]\n",
    "\n",
    "subs = os.listdir(train_root)\n",
    "for sub in subs:\n",
    "    abs_sub = os.path.join(train_root, sub)\n",
    "    files = os.listdir(abs_sub)\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    for i in tqdm_notebook(range(img_count)):\n",
    "        img = cv2.imdecode(np.fromfile(os.path.join(abs_sub, files[i]), dtype=np.uint8), -1)\n",
    "        img = cv2.resize(img,(img_h,img_w))\n",
    "        img = img[:,:,:,np.newaxis]\n",
    "\n",
    "        imgs = np.concatenate((imgs,img),axis=3)\n",
    "        \n",
    "        \n",
    "imgs = imgs[:,:,:,1:]\n",
    "print(imgs.shape)\n",
    "imgs = imgs.astype(np.float32)/255.\n",
    " \n",
    "for i in tqdm_notebook(range(3)):\n",
    "    pixels = imgs[:,i,:].ravel() # pull in a row\n",
    "    means.append(round(np.mean(pixels), 4))\n",
    "    stdevs.append(round(np.std(pixels), 4))\n",
    " \n",
    "means.reverse() # BGR --> RGB\n",
    "stdevs.reverse()\n",
    " \n",
    "print(\"normMean = {}\".format(means))\n",
    "print(\"normStd = {}\".format(stdevs))\n",
    "print('transforms.Normalize(normMean = {},normStd = {})'.format(means,stdevs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44d2ba",
   "metadata": {},
   "source": [
    "## Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13cf106a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:58:38.788447Z",
     "start_time": "2023-06-13T01:58:38.771493Z"
    }
   },
   "outputs": [],
   "source": [
    "imgs_dir = r\"~\\data\\imgs_feature_extraction_deep_learning\\imgs_tone\\train\"\n",
    "n_labels = len(os.listdir(imgs_dir))\n",
    "\n",
    "label_dir = os.path.split(imgs_dir)[0]+'\\\\labels'\n",
    "if not os.path.exists(label_dir):\n",
    "    os.makedirs(label_dir)\n",
    "\n",
    "imgs_0_dir = os.path.join(imgs_dir, '0')\n",
    "imgs_1_dir = os.path.join(imgs_dir, '1')\n",
    "\n",
    "file_0_all = os.listdir(imgs_0_dir)\n",
    "file_1_all = os.listdir(imgs_1_dir)\n",
    "\n",
    "random.shuffle(file_1_all)\n",
    "random.shuffle(file_0_all)\n",
    "\n",
    "\n",
    "if n_labels == 3:\n",
    "    imgs_2_dir = os.path.join(imgs_dir, '2')\n",
    "    file_2_all = os.listdir(imgs_2_dir)\n",
    "    random.shuffle(file_2_all)\n",
    "    \n",
    "\n",
    "print(len(file_0_all),len(file_1_all),len(file_2_all)) if n_labels == 3  else print(len(file_0_all),len(file_1_all))\n",
    "\n",
    "for i in range(1, 11):\n",
    "    rate = i * 0.1\n",
    "    file_0_list = file_0_all[:int(len(file_0_all)*rate)]\n",
    "    file_1_list = file_1_all[:int(len(file_1_all)*rate)]\n",
    "\n",
    "    for j in range(len(file_0_list)):\n",
    "        file_0_list[j] = imgs_dir + '\\\\0\\\\' + file_0_list[j] + ',0'\n",
    "    for k in range(len(file_1_list)):\n",
    "        file_1_list[k] = imgs_dir + '\\\\1\\\\' + file_1_list[k] + ',1'\n",
    "    \n",
    "    label_list = file_0_list[:int(len(file_0_list)*0.9)] + file_1_list[:int(len(file_1_list)*0.9)]\n",
    "    val_list = file_0_list[int(len(file_0_list)*0.9):] + file_1_list[int(len(file_1_list)*0.9):]\n",
    "    \n",
    "    if n_labels == 3:\n",
    "        file_2_list = file_2_all[:int(len(file_2_all)*rate)]\n",
    "        \n",
    "        for q in range(len(file_2_list)):\n",
    "            file_2_list[q] = imgs_dir + '\\\\2\\\\' + file_2_list[q] + ',2'\n",
    "            \n",
    "        label_list += file_2_list[:int(len(file_2_list)*0.9)]\n",
    "        val_list += file_2_list[int(len(file_2_list)*0.9):]\n",
    "    \n",
    "    if n_labels == 3:\n",
    "        print(len(label_list), len(val_list), len(file_0_list), len(file_1_list), len(file_2_list)) \n",
    "    else:\n",
    "        print(len(label_list), len(val_list), len(file_0_list), len(file_1_list))\n",
    "    \n",
    "\n",
    "    label_txt_path = os.path.join(label_dir, 'label_'+str(i*10)+'.txt')\n",
    "    val_label_txt_path = os.path.join(label_dir, 'val_label_'+str(i*10)+'.txt')\n",
    "\n",
    "\n",
    "    with open(label_txt_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for l in range(len(label_list)):\n",
    "            f.write(label_list[l]+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "    with open(val_label_txt_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for m in range(len(val_list)):\n",
    "            f.write(val_list[m]+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "for i in range(1, 11):\n",
    "    train_list = label_list+val_list\n",
    "\n",
    "    val_label_txt_path = os.path.join(label_dir, 'val_label_'+str(i*10)+'.txt')\n",
    "    train_txt_path = os.path.join(label_dir, 'train_'+str(i*10)+'.txt')\n",
    "    \n",
    "    f_val = open(val_label_txt_path, 'r', encoding='utf-8')\n",
    "    val_file_list = f_val.readlines()\n",
    "    for val_file in val_file_list:\n",
    "        if val_file[:-1] in train_list:\n",
    "\n",
    "            train_list.remove(val_file[:-1])\n",
    "\n",
    "    with open(train_txt_path, 'w', encoding=\"utf-8\") as f_train:\n",
    "        for n in range(len(train_list)):\n",
    "            f_train.write(train_list[n]+'\\n')\n",
    "        f_train.close()\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be9b652f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:58:46.862550Z",
     "start_time": "2023-06-13T01:58:46.848588Z"
    }
   },
   "outputs": [],
   "source": [
    "# randomly select 400 images from each group, calculate the mean and std\n",
    "\n",
    "root = os.path.split(os.path.split(imgs_dir)[0])[0]\n",
    "train_root = root + r'\\imgs_tone\\train'\n",
    "val_path = root + r'\\imgs_tone'\n",
    "subs = os.listdir(train_root)\n",
    "min_imgs = 10000000\n",
    "subs = os.listdir(train_root)\n",
    "for sub in subs:\n",
    "    abs_sub = os.path.join(train_root, sub)\n",
    "    files = os.listdir(abs_sub)\n",
    "    count = len(files)\n",
    "    print(sub, ':', count)\n",
    "    if count < min_imgs:\n",
    "        min_imgs = count       \n",
    "img_count = min(400, min_imgs)\n",
    "print('to chk :', img_count)\n",
    "img_h,img_w = 224,224\n",
    "imgs = np.zeros([img_w,img_h,3,1])\n",
    "means,stdevs = [],[]\n",
    "\n",
    "subs = os.listdir(train_root)\n",
    "for sub in subs:\n",
    "    abs_sub = os.path.join(train_root, sub)\n",
    "    files = os.listdir(abs_sub)\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    for i in tqdm_notebook(range(img_count)):\n",
    "        img = cv2.imdecode(np.fromfile(os.path.join(abs_sub, files[i]), dtype=np.uint8), -1)\n",
    "        img = cv2.resize(img,(img_h,img_w))\n",
    "        img = img[:,:,:,np.newaxis]\n",
    "\n",
    "        imgs = np.concatenate((imgs,img),axis=3)\n",
    "        \n",
    "        \n",
    "imgs = imgs[:,:,:,1:]\n",
    "print(imgs.shape)\n",
    "imgs = imgs.astype(np.float32)/255.\n",
    " \n",
    "for i in tqdm_notebook(range(3)):\n",
    "    pixels = imgs[:,i,:].ravel() # pull in a row\n",
    "    means.append(round(np.mean(pixels), 4))\n",
    "    stdevs.append(round(np.std(pixels), 4))\n",
    " \n",
    "means.reverse() # BGR --> RGB\n",
    "stdevs.reverse()\n",
    " \n",
    "print(\"normMean = {}\".format(means))\n",
    "print(\"normStd = {}\".format(stdevs))\n",
    "print('transforms.Normalize(normMean = {},normStd = {})'.format(means,stdevs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5391e155",
   "metadata": {},
   "source": [
    "## Elevated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63709c6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:59:38.117635Z",
     "start_time": "2023-06-13T01:59:38.111623Z"
    }
   },
   "outputs": [],
   "source": [
    "imgs_dir = r\"~\\data\\imgs_feature_extraction_deep_learning\\imgs_elevated\\train\"\n",
    "n_labels = len(os.listdir(imgs_dir))\n",
    "\n",
    "\n",
    "label_dir = os.path.split(imgs_dir)[0]+'\\\\labels'\n",
    "if not os.path.exists(label_dir):\n",
    "    os.makedirs(label_dir)\n",
    "\n",
    "\n",
    "imgs_0_dir = os.path.join(imgs_dir, '0')\n",
    "imgs_1_dir = os.path.join(imgs_dir, '1')\n",
    "\n",
    "file_0_all = os.listdir(imgs_0_dir)\n",
    "file_1_all = os.listdir(imgs_1_dir)\n",
    "\n",
    "random.shuffle(file_1_all)\n",
    "random.shuffle(file_0_all)\n",
    "\n",
    "\n",
    "if n_labels == 3:\n",
    "    imgs_2_dir = os.path.join(imgs_dir, '2')\n",
    "    file_2_all = os.listdir(imgs_2_dir)\n",
    "    random.shuffle(file_2_all)\n",
    "    \n",
    "\n",
    "print(len(file_0_all),len(file_1_all),len(file_2_all)) if n_labels == 3  else print(len(file_0_all),len(file_1_all))\n",
    "\n",
    "for i in range(1, 11):\n",
    "    rate = i * 0.1\n",
    "    file_0_list = file_0_all[:int(len(file_0_all)*rate)]\n",
    "    file_1_list = file_1_all[:int(len(file_1_all)*rate)]\n",
    "\n",
    "    for j in range(len(file_0_list)):\n",
    "        file_0_list[j] = imgs_dir + '\\\\0\\\\' + file_0_list[j] + ',0'\n",
    "    for k in range(len(file_1_list)):\n",
    "        file_1_list[k] = imgs_dir + '\\\\1\\\\' + file_1_list[k] + ',1'\n",
    "    \n",
    "    label_list = file_0_list[:int(len(file_0_list)*0.9)] + file_1_list[:int(len(file_1_list)*0.9)]\n",
    "    val_list = file_0_list[int(len(file_0_list)*0.9):] + file_1_list[int(len(file_1_list)*0.9):]\n",
    "    \n",
    "    if n_labels == 3:\n",
    "        file_2_list = file_2_all[:int(len(file_2_all)*rate)]\n",
    "        \n",
    "        for q in range(len(file_2_list)):\n",
    "            file_2_list[q] = imgs_dir + '\\\\2\\\\' + file_2_list[q] + ',2'\n",
    "            \n",
    "        label_list += file_2_list[:int(len(file_2_list)*0.9)]\n",
    "        val_list += file_2_list[int(len(file_2_list)*0.9):]\n",
    "    \n",
    "    if n_labels == 3:\n",
    "        print(len(label_list), len(val_list), len(file_0_list), len(file_1_list), len(file_2_list)) \n",
    "    else:\n",
    "        print(len(label_list), len(val_list), len(file_0_list), len(file_1_list))\n",
    "    \n",
    "\n",
    "    label_txt_path = os.path.join(label_dir, 'label_'+str(i*10)+'.txt')\n",
    "    val_label_txt_path = os.path.join(label_dir, 'val_label_'+str(i*10)+'.txt')\n",
    "\n",
    "\n",
    "    with open(label_txt_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for l in range(len(label_list)):\n",
    "            f.write(label_list[l]+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "\n",
    "    with open(val_label_txt_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for m in range(len(val_list)):\n",
    "            f.write(val_list[m]+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "\n",
    "for i in range(1, 11):\n",
    "    train_list = label_list+val_list\n",
    "\n",
    "    val_label_txt_path = os.path.join(label_dir, 'val_label_'+str(i*10)+'.txt')\n",
    "    train_txt_path = os.path.join(label_dir, 'train_'+str(i*10)+'.txt')\n",
    "    \n",
    "    f_val = open(val_label_txt_path, 'r', encoding='utf-8')\n",
    "    val_file_list = f_val.readlines()\n",
    "    for val_file in val_file_list:\n",
    "        if val_file[:-1] in train_list:\n",
    "\n",
    "            train_list.remove(val_file[:-1])\n",
    "\n",
    "    with open(train_txt_path, 'w', encoding=\"utf-8\") as f_train:\n",
    "        for n in range(len(train_list)):\n",
    "            f_train.write(train_list[n]+'\\n')\n",
    "        f_train.close()\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bba04b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:59:48.743851Z",
     "start_time": "2023-06-13T01:59:48.736870Z"
    }
   },
   "outputs": [],
   "source": [
    "# randomly select 400 images from each group, calculate the mean and std\n",
    "\n",
    "root = os.path.split(os.path.split(imgs_dir)[0])[0]\n",
    "train_root = root + r'\\imgs_elevated\\train'\n",
    "val_path = root + r'\\imgs_elevated'\n",
    "subs = os.listdir(train_root)\n",
    "min_imgs = 10000000\n",
    "subs = os.listdir(train_root)\n",
    "for sub in subs:\n",
    "    abs_sub = os.path.join(train_root, sub)\n",
    "    files = os.listdir(abs_sub)\n",
    "    count = len(files)\n",
    "    print(sub, ':', count)\n",
    "    if count < min_imgs:\n",
    "        min_imgs = count       \n",
    "img_count = min(400, min_imgs)\n",
    "print('to chk :', img_count)\n",
    "img_h,img_w = 224,224\n",
    "imgs = np.zeros([img_w,img_h,3,1])\n",
    "means,stdevs = [],[]\n",
    "\n",
    "subs = os.listdir(train_root)\n",
    "for sub in subs:\n",
    "    abs_sub = os.path.join(train_root, sub)\n",
    "    files = os.listdir(abs_sub)\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    for i in tqdm_notebook(range(img_count)):\n",
    "        img = cv2.imdecode(np.fromfile(os.path.join(abs_sub, files[i]), dtype=np.uint8), -1)\n",
    "        img = cv2.resize(img,(img_h,img_w))\n",
    "        img = img[:,:,:,np.newaxis]\n",
    "\n",
    "        imgs = np.concatenate((imgs,img),axis=3)\n",
    "        \n",
    "        \n",
    "imgs = imgs[:,:,:,1:]\n",
    "print(imgs.shape)\n",
    "imgs = imgs.astype(np.float32)/255.\n",
    " \n",
    "for i in tqdm_notebook(range(3)):\n",
    "    pixels = imgs[:,i,:].ravel() # pull in a row\n",
    "    means.append(round(np.mean(pixels), 4))\n",
    "    stdevs.append(round(np.std(pixels), 4))\n",
    " \n",
    "means.reverse() # BGR --> RGB\n",
    "stdevs.reverse()\n",
    " \n",
    "print(\"normMean = {}\".format(means))\n",
    "print(\"normStd = {}\".format(stdevs))\n",
    "print('transforms.Normalize(normMean = {},normStd = {})'.format(means,stdevs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0e591",
   "metadata": {},
   "source": [
    "### Depressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ed05698",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T02:00:10.080203Z",
     "start_time": "2023-06-13T02:00:10.073222Z"
    }
   },
   "outputs": [],
   "source": [
    "imgs_dir = r\"~\\data\\imgs_feature_extraction_deep_learning\\imgs_depressed\\train\"\n",
    "n_labels = len(os.listdir(imgs_dir))\n",
    "\n",
    "\n",
    "label_dir = os.path.split(imgs_dir)[0]+'\\\\labels'\n",
    "if not os.path.exists(label_dir):\n",
    "    os.makedirs(label_dir)\n",
    "\n",
    "\n",
    "imgs_0_dir = os.path.join(imgs_dir, '0')\n",
    "imgs_1_dir = os.path.join(imgs_dir, '1')\n",
    "\n",
    "file_0_all = os.listdir(imgs_0_dir)\n",
    "file_1_all = os.listdir(imgs_1_dir)\n",
    "\n",
    "random.shuffle(file_1_all)\n",
    "random.shuffle(file_0_all)\n",
    "\n",
    "\n",
    "if n_labels == 3:\n",
    "    imgs_2_dir = os.path.join(imgs_dir, '2')\n",
    "    file_2_all = os.listdir(imgs_2_dir)\n",
    "    random.shuffle(file_2_all)\n",
    "    \n",
    "\n",
    "print(len(file_0_all),len(file_1_all),len(file_2_all)) if n_labels == 3  else print(len(file_0_all),len(file_1_all))\n",
    "\n",
    "for i in range(1, 11):\n",
    "    rate = i * 0.1\n",
    "    file_0_list = file_0_all[:int(len(file_0_all)*rate)]\n",
    "    file_1_list = file_1_all[:int(len(file_1_all)*rate)]\n",
    "\n",
    "    for j in range(len(file_0_list)):\n",
    "        file_0_list[j] = imgs_dir + '\\\\0\\\\' + file_0_list[j] + ',0'\n",
    "    for k in range(len(file_1_list)):\n",
    "        file_1_list[k] = imgs_dir + '\\\\1\\\\' + file_1_list[k] + ',1'\n",
    "    \n",
    "    label_list = file_0_list[:int(len(file_0_list)*0.9)] + file_1_list[:int(len(file_1_list)*0.9)]\n",
    "    val_list = file_0_list[int(len(file_0_list)*0.9):] + file_1_list[int(len(file_1_list)*0.9):]\n",
    "    \n",
    "    if n_labels == 3:\n",
    "        file_2_list = file_2_all[:int(len(file_2_all)*rate)]\n",
    "        \n",
    "        for q in range(len(file_2_list)):\n",
    "            file_2_list[q] = imgs_dir + '\\\\2\\\\' + file_2_list[q] + ',2'\n",
    "            \n",
    "        label_list += file_2_list[:int(len(file_2_list)*0.9)]\n",
    "        val_list += file_2_list[int(len(file_2_list)*0.9):]\n",
    "    \n",
    "    if n_labels == 3:\n",
    "        print(len(label_list), len(val_list), len(file_0_list), len(file_1_list), len(file_2_list)) \n",
    "    else:\n",
    "        print(len(label_list), len(val_list), len(file_0_list), len(file_1_list))\n",
    "    \n",
    "\n",
    "    label_txt_path = os.path.join(label_dir, 'label_'+str(i*10)+'.txt')\n",
    "    val_label_txt_path = os.path.join(label_dir, 'val_label_'+str(i*10)+'.txt')\n",
    "\n",
    "    with open(label_txt_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for l in range(len(label_list)):\n",
    "            f.write(label_list[l]+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "    with open(val_label_txt_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for m in range(len(val_list)):\n",
    "            f.write(val_list[m]+'\\n')\n",
    "        f.close()\n",
    "        \n",
    "\n",
    "for i in range(1, 11):\n",
    "    train_list = label_list+val_list\n",
    "\n",
    "    val_label_txt_path = os.path.join(label_dir, 'val_label_'+str(i*10)+'.txt')\n",
    "    train_txt_path = os.path.join(label_dir, 'train_'+str(i*10)+'.txt')\n",
    "    \n",
    "    f_val = open(val_label_txt_path, 'r', encoding='utf-8')\n",
    "    val_file_list = f_val.readlines()\n",
    "    for val_file in val_file_list:\n",
    "        if val_file[:-1] in train_list:\n",
    "\n",
    "            train_list.remove(val_file[:-1])\n",
    "\n",
    "    with open(train_txt_path, 'w', encoding=\"utf-8\") as f_train:\n",
    "        for n in range(len(train_list)):\n",
    "            f_train.write(train_list[n]+'\\n')\n",
    "        f_train.close()\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3870eef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T02:00:22.799631Z",
     "start_time": "2023-06-13T02:00:22.784619Z"
    }
   },
   "outputs": [],
   "source": [
    "# randomly select 400 images from each group, calculate the mean and std\n",
    "\n",
    "root = os.path.split(os.path.split(imgs_dir)[0])[0]\n",
    "train_root = root + r'\\imgs_depressed\\train'\n",
    "val_path = root + r'\\imgs_depressed'\n",
    "subs = os.listdir(train_root)\n",
    "min_imgs = 10000000\n",
    "subs = os.listdir(train_root)\n",
    "for sub in subs:\n",
    "    abs_sub = os.path.join(train_root, sub)\n",
    "    files = os.listdir(abs_sub)\n",
    "    count = len(files)\n",
    "    print(sub, ':', count)\n",
    "    if count < min_imgs:\n",
    "        min_imgs = count       \n",
    "img_count = min(400, min_imgs)\n",
    "print('to chk :', img_count)\n",
    "img_h,img_w = 224,224\n",
    "imgs = np.zeros([img_w,img_h,3,1])\n",
    "means,stdevs = [],[]\n",
    "\n",
    "subs = os.listdir(train_root)\n",
    "for sub in subs:\n",
    "    abs_sub = os.path.join(train_root, sub)\n",
    "    files = os.listdir(abs_sub)\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    for i in tqdm_notebook(range(img_count)):\n",
    "        img = cv2.imdecode(np.fromfile(os.path.join(abs_sub, files[i]), dtype=np.uint8), -1)\n",
    "        img = cv2.resize(img,(img_h,img_w))\n",
    "        img = img[:,:,:,np.newaxis]\n",
    "\n",
    "        imgs = np.concatenate((imgs,img),axis=3)\n",
    "        \n",
    "        \n",
    "imgs = imgs[:,:,:,1:]\n",
    "print(imgs.shape)\n",
    "imgs = imgs.astype(np.float32)/255.\n",
    " \n",
    "for i in tqdm_notebook(range(3)):\n",
    "    pixels = imgs[:,i,:].ravel() # pull in a row\n",
    "    means.append(round(np.mean(pixels), 4))\n",
    "    stdevs.append(round(np.std(pixels), 4))\n",
    " \n",
    "means.reverse() # BGR --> RGB\n",
    "stdevs.reverse()\n",
    " \n",
    "print(\"normMean = {}\".format(means))\n",
    "print(\"normStd = {}\".format(stdevs))\n",
    "print('transforms.Normalize(normMean = {},normStd = {})'.format(means,stdevs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31cb033",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
