{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0472fd24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T02:09:18.277571Z",
     "start_time": "2023-06-13T02:09:18.270590Z"
    }
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.insert(0, r'~\\trainer')\n",
    "sys.path.insert(0, r'~\\core')\n",
    "\n",
    "import time\n",
    "from natsort import natsorted\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core.mean_teacher_main import TextFileDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa684ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T05:32:08.265016Z",
     "start_time": "2023-03-17T05:32:08.251146Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_all_files(from_dir, followlinks=True, file_exts=None, exclude_file_exts=None):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(from_dir, followlinks=followlinks):\n",
    "        for name in files:\n",
    "            if file_exts:\n",
    "                _, ext = os.path.splitext(name)\n",
    "                if ext not in file_exts:\n",
    "                    continue\n",
    "\n",
    "            if exclude_file_exts:\n",
    "                _, ext = os.path.splitext(name)\n",
    "                if ext in exclude_file_exts:\n",
    "                    continue\n",
    "            path_join = os.path.join(root, name)\n",
    "            all_files.append(path_join)\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d2e9ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T05:32:08.312957Z",
     "start_time": "2023-03-17T05:32:08.266015Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    fc_inputs = model.fc.in_features\n",
    "    model.fc = nn.Linear(fc_inputs, 2)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "def get_roc(y_true, predicts, to_check_path_result, threshold_num=20, to_print=True):\n",
    "    print(len(y_true), len(predicts))\n",
    "    df = pd.DataFrame({'y_true':y_true, 'predicts':predicts})\n",
    "    df.to_csv(os.path.join(to_check_path_result,'result.csv'), encoding='utf-8', index=False, sep=',')\n",
    "    pred_list = [ i / 100 for i in predicts]\n",
    "    data = list(zip(pred_list, y_true))\n",
    "    thresholds = [i / threshold_num for i in range(0, threshold_num, 1)]\n",
    "    thresholds.append(1)\n",
    "    tp = []\n",
    "    fp = []\n",
    "    fn = []\n",
    "    tn = []\n",
    "    tpr = []\n",
    "    tnr = []\n",
    "    fpr = []\n",
    "    for thrd in thresholds:\n",
    "        thrd_tp, thrd_fp, thrd_fn, thrd_tn = [0] * 4\n",
    "        for item in data:\n",
    "            if int(item[1]) == 1:\n",
    "                if item[0] > thrd:\n",
    "                    thrd_tp += 1\n",
    "                else:\n",
    "                    thrd_fn += 1\n",
    "            elif int(item[1]) == 0:\n",
    "                if item[0] > thrd:\n",
    "                    thrd_fp += 1\n",
    "                else:\n",
    "                    thrd_tn += 1\n",
    "        thrd_tpr = round(float(thrd_tp) / (thrd_tp + thrd_fn), 3)\n",
    "        thrd_fpr = round(float(thrd_fp) / (thrd_tn + thrd_fp), 3)\n",
    "        print(thrd, thrd_tp, thrd_tp + thrd_fn, thrd_tpr)\n",
    "        print(thrd, thrd_fp, thrd_fp + thrd_tn, thrd_fpr)\n",
    "        tp.append(thrd_tp)\n",
    "        fp.append(thrd_fp)\n",
    "        fn.append(thrd_fn)\n",
    "        tn.append(thrd_tn)\n",
    "        tpr.append(thrd_tpr)\n",
    "        tnr.append(round(1-thrd_fpr, 3))\n",
    "        fpr.append(thrd_fpr)\n",
    "    diff = [round(tpr[i] - fpr[i], 3) for i in range(len(tpr))]\n",
    "    optimal_idx = np.argmax(diff)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    optimal_acc0 = round(tn[optimal_idx] / (tn[optimal_idx] + fp[optimal_idx]), 3) * 100\n",
    "    optimal_acc1 = round(tp[optimal_idx] / (tp[optimal_idx] + fn[optimal_idx]), 3) * 100\n",
    "    optimal_avg_acc = np.mean([optimal_acc0, optimal_acc1])\n",
    "    optimal_overall_acc = round((tn[optimal_idx] + tp[optimal_idx]) / len(y_true), 3) * 100\n",
    "\n",
    "    print('optimal_threshold: ', optimal_threshold, ' overall acc:  %.2f%%, avg acc: %.2f%%' % (optimal_overall_acc, optimal_avg_acc))\n",
    "\n",
    "    if to_print:\n",
    "        print(\"{}\\t{}\\t{}\\t{}\\t{}\".format('thred', 'tpr', 'tnr', 'fpr', 'diff'))\n",
    "        for i, thrd in enumerate(thresholds):\n",
    "            print('{}\\t{}\\t{}\\t{}\\t{}'.format(thresholds[i], tpr[i], tnr[i], fpr[i], diff[i]))\n",
    "\n",
    "    acc0 = [round(item / (item + fp[i]), 3) * 100 for i, item in enumerate(tn)]\n",
    "    acc1 = [round(item / (item + fn[i]), 3) * 100 for i, item in enumerate(tp)]\n",
    "\n",
    "    df = pd.DataFrame({'thresholds': thresholds, 'tpr': tpr, 'tnr': tnr, 'fpr': fpr, 'tpr-fpr': diff,\n",
    "                       'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp,\n",
    "                       'acc0': acc0, 'acc1': acc1})\n",
    "    try:\n",
    "        df = df.ix[:, ['thresholds', 'tpr', 'tnr', 'fpr', 'tpr-fpr', 'tn', 'fp', 'fn', 'tp', 'acc0', 'acc1']]\n",
    "    except:\n",
    "        df = df.loc[:, ['thresholds', 'tpr', 'tnr', 'fpr', 'tpr-fpr', 'tn', 'fp', 'fn', 'tp', 'acc0', 'acc1']]\n",
    "    df.to_csv(to_check_path_result + r'\\roc_%s_%s.csv' % (optimal_threshold, optimal_avg_acc), encoding='utf-8')\n",
    "\n",
    "    fontsize = 18\n",
    "    ax = plt.figure(figsize=(10, 8)) \n",
    "    plt.plot(fpr, tpr, lw=2) \n",
    "    plt.ylabel('sensitivity', fontdict={'family': 'Times New Roman', 'size': fontsize+2})\n",
    "    plt.xlabel('1-specificity', fontdict={'family': 'Times New Roman', 'size': fontsize+2})\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    new_ticks = np.linspace(0, 1, 11)\n",
    "    plt.xticks(new_ticks, fontproperties='Times New Roman', fontsize=fontsize)\n",
    "    plt.yticks(new_ticks, fontproperties='Times New Roman', fontsize=fontsize)\n",
    "    plt.tick_params(labelsize=fontsize)\n",
    "\n",
    "    plt.annotate(r'threshold={:.3f}'.format(optimal_threshold), xy=(fpr[optimal_idx], tpr[optimal_idx]),\n",
    "                 xycoords='data', xytext=(+30, -30),\n",
    "                 textcoords='offset points', fontsize=fontsize, color='blue',\n",
    "                 arrowprops=dict(arrowstyle='->', connectionstyle=\"arc3,rad=.1\", color='red'))\n",
    "\n",
    "    plt.savefig(to_check_path_result + r'\\roc_%s_%s.jpg' % (optimal_threshold, optimal_avg_acc))\n",
    "    \n",
    "    \n",
    "    auc_val = auc(fpr, tpr)\n",
    "    \n",
    "    draw_roc(auc_val, fpr, tpr, to_check_path_result)\n",
    "    return auc_val,optimal_acc0,optimal_acc1,optimal_overall_acc, optimal_avg_acc,optimal_threshold\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], \n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def show_matrix(y_pred, y_true, classes_count, out_put_dir, fig_size=4, dpi=110, savefig=True):\n",
    "\n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(fig_size, fig_size), dpi=dpi)\n",
    "    classes = [str(x) for x in range(classes_count)]\n",
    "    plot_confusion_matrix(cnf_matrix, classes=classes,\n",
    "                               title='Confusion matrix')\n",
    "\n",
    "    rand = random.randint(1, 1000)\n",
    "    file_name = time.strftime(\"%Y-%m-%d-%H-%M_\", time.localtime()) + str(rand) + \".jpg\"\n",
    "    file_path = os.path.abspath(out_put_dir + '/' + file_name)\n",
    "    if savefig:\n",
    "        plt.savefig(file_path)\n",
    "    err = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        if y_pred[i] != y_true[i]:\n",
    "            err += 1\n",
    "\n",
    "    overall_acc = 1 - err * 1.0 / len(y_pred)\n",
    "    print(cnf_matrix)\n",
    "    acc_list = []\n",
    "    for i in range(cnf_matrix.shape[0]):\n",
    "        acc = 100 * cnf_matrix[i, i] / np.sum(cnf_matrix[i, :])\n",
    "        print('%02d acc: %.2f%%' % (i, acc))\n",
    "        acc_list.append(acc)\n",
    "    print('overall acc: %.2f%%, avg acc: %.2f%%' % (100 * overall_acc, np.mean(acc_list)))\n",
    "    \n",
    "def draw_roc(roc_auc, fpr, tpr, to_check_path_result):\n",
    "    fs = 16\n",
    "    print(roc_auc, fpr, tpr)\n",
    "    plt.subplots(figsize=(8,6));\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=2, label='ROC curve (area = %0.4f)' % roc_auc);\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--');\n",
    "    plt.xlim([0.0, 1.0]);\n",
    "    plt.ylim([0.0, 1.05]);\n",
    "    plt.xticks(fontsize=fs)\n",
    "    plt.yticks(fontsize=fs)\n",
    "    plt.xlabel('False Positive Rate',fontsize=fs);\n",
    "    plt.ylabel('True Positive Rate',fontsize=fs);\n",
    "    plt.title('ROC Curve',fontsize=fs);\n",
    "    plt.legend(loc=\"lower right\",fontsize=fs);\n",
    "    plt.savefig(os.path.join(to_check_path_result, 'roc1.jpg'))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def predict_path(model, to_chk_path, add_t=''):\n",
    "    model.eval() \n",
    "    total_pred = []\n",
    "    total_true = np.array([])\n",
    "    total_zxd = []\n",
    "    to_check_path_result = to_chk_path + '_result_%s' % add_t +'_'+ time.strftime(\"%y%m%d_%H%M%S\", time.localtime(time.time()))      \n",
    "    if not os.path.exists(to_check_path_result):\n",
    "            os.makedirs(to_check_path_result)\n",
    "    with torch.no_grad():\n",
    "        for X, y in eval_loader:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda(non_blocking=True)\n",
    "            score = model(X)\n",
    "            _, prediction = torch.max(score, 1)\n",
    "            percentage = torch.nn.functional.softmax(score, dim=1) * 100\n",
    "            percentage_list = percentage.cpu().detach().numpy().tolist()\n",
    "            pred_cls = [item.index(max(item)) for item in percentage_list]\n",
    "            cls1_zxd = [item[1] for item in percentage_list]\n",
    "            total_pred.extend(pred_cls)\n",
    "            total_true = np.concatenate((total_true, y.data.cpu()))\n",
    "            total_zxd.extend(cls1_zxd)\n",
    "    auc_val, optimal_acc0,optimal_acc1,optimal_overall_acc, optimal_avg_acc,optimal_threshold = get_roc(total_true.tolist(), total_zxd, to_check_path_result)\n",
    "    show_matrix(total_pred, total_true.tolist(), 2, to_check_path_result)\n",
    "    return [auc_val,optimal_acc0,optimal_acc1,optimal_overall_acc, optimal_avg_acc,optimal_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b00d73",
   "metadata": {},
   "source": [
    "## Clear boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe37e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T02:12:06.348214Z",
     "start_time": "2023-06-13T02:12:06.337244Z"
    }
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "test_dir = r\"~\\data\\imgs_feature_extraction_deep_learning\\imgs_boundary_clear\\test\"\n",
    "test_txt_path = os.path.join(test_dir, 'test.txt')\n",
    "test_file_list_0 = []\n",
    "test_file_list_1 = []\n",
    "dirnames = fetch_all_files(test_dir, file_exts = [\".jpg\", '.jpeg'])\n",
    "for i in range(len(dirnames)):\n",
    "    if os.path.split(dirnames[i])[0][-1] == '0':\n",
    "        test_file_list_0.append(dirnames[i])\n",
    "    elif os.path.split(dirnames[i])[0][-1] == '1':\n",
    "        test_file_list_1.append(dirnames[i])\n",
    "        \n",
    "with open(test_txt_path, 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(test_file_list_0)):\n",
    "        f.write(test_file_list_0[i]+',0'+'\\n')\n",
    "    for i in range(len(test_file_list_1)):\n",
    "        f.write(test_file_list_1[i]+',1'+'\\n')\n",
    "f.close()\n",
    "\n",
    "\n",
    "# copied from the model_training_script\n",
    "channel_stats = dict(mean=[0.2061, 0.1973, 0.1918],std=[0.2777, 0.2747, 0.2733])\n",
    "test_dataset = test_txt_path\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "test_rst_pre = 'model_path'\n",
    "model_root = test_rst_pre\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "subs = [f for f in os.listdir(model_root) if os.path.isdir(os.path.join(model_root, f))] \n",
    "subs = natsorted(subs)\n",
    "\n",
    "eval_transformation = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**channel_stats)\n",
    "    ])\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "        TextFileDataset(test_dataset, eval_transformation),\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=2 * 2,\n",
    "        pin_memory=True,\n",
    "        drop_last=False)\n",
    "\n",
    "\n",
    "auc_list = []\n",
    "optimal_th_list = []\n",
    "acc_best0 = []\n",
    "acc_best1 = []\n",
    "acc_best_ov = []\n",
    "acc_best_avg = []\n",
    "rate_list = []\n",
    "\n",
    "all_list = [auc_list,acc_best0,acc_best1,acc_best_ov,acc_best_avg,optimal_th_list]\n",
    "\n",
    "for sub in subs:\n",
    "    PATH = os.path.join(model_root,sub,'transient', 'model_checkpoint.pth')\n",
    "    EMA_PATH = os.path.join(model_root,sub,'transient_ema', 'model_checkpoint.pth')\n",
    "    model = load_model(PATH)\n",
    "    ema_model = load_model(EMA_PATH)\n",
    "\n",
    "    add_str=sub.split('_')[1]\n",
    "\n",
    "    rst_list = predict_path(model, test_rst_pre, add_str+'base')\n",
    "    ema_rst_list = predict_path(ema_model, test_rst_pre, add_str+'ema')\n",
    "    \n",
    "    rate_list.append(add_str[1:])\n",
    "    \n",
    "    if rst_list[0] > ema_rst_list[0]:\n",
    "        for ii,item in enumerate(all_list):\n",
    "            item.append(rst_list[ii])\n",
    "    else:\n",
    "        for ii,item in enumerate(all_list):\n",
    "            item.append(ema_rst_list[ii])\n",
    "\n",
    "df = pd.DataFrame({'rate':rate_list, 'auc':auc_list, 'best_acc0':acc_best0, 'best_acc1':acc_best1, \n",
    "                   'best_acc_ov':acc_best_ov, 'best_acc_avg':acc_best_avg, 'best_th':optimal_th_list})\n",
    "df.to_csv(os.path.join(model_root,'result.csv'), encoding='utf-8', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d8b72",
   "metadata": {},
   "source": [
    "## Surface_rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44266ea3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T02:12:29.391205Z",
     "start_time": "2023-06-13T02:12:29.375248Z"
    }
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "test_dir = r\"~\\data\\imgs_feature_extraction_deep_learning\\imgs_surface_rough\\test\"\n",
    "\n",
    "\n",
    "test_txt_path = os.path.join(test_dir, 'test.txt')\n",
    "test_file_list_0 = []\n",
    "test_file_list_1 = []\n",
    "dirnames = fetch_all_files(test_dir, file_exts = [\".jpg\", '.jpeg'])\n",
    "for i in range(len(dirnames)):\n",
    "    if os.path.split(dirnames[i])[0][-1] == '0':\n",
    "        test_file_list_0.append(dirnames[i])\n",
    "    elif os.path.split(dirnames[i])[0][-1] == '1':\n",
    "        test_file_list_1.append(dirnames[i])\n",
    "        \n",
    "with open(test_txt_path, 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(test_file_list_0)):\n",
    "        f.write(test_file_list_0[i]+',0'+'\\n')\n",
    "    for i in range(len(test_file_list_1)):\n",
    "        f.write(test_file_list_1[i]+',1'+'\\n')\n",
    "f.close()\n",
    "\n",
    "\n",
    "channel_stats = dict(mean=[0.1801, 0.1735, 0.1492],std=[0.2437, 0.2395, 0.2168])\n",
    "test_dataset = test_txt_path\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "test_rst_pre = 'model_path'\n",
    "model_root = test_rst_pre\n",
    "#########################################################################################################################\n",
    "\n",
    "\n",
    "subs = [f for f in os.listdir(model_root) if os.path.isdir(os.path.join(model_root, f))] \n",
    "subs = natsorted(subs)\n",
    "\n",
    "eval_transformation = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**channel_stats)\n",
    "    ])\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "        TextFileDataset(test_dataset, eval_transformation),\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=2 * 2,  \n",
    "        pin_memory=True,\n",
    "        drop_last=False)\n",
    "\n",
    "\n",
    "auc_list = []\n",
    "optimal_th_list = []\n",
    "acc_best0 = []\n",
    "acc_best1 = []\n",
    "acc_best_ov = []\n",
    "acc_best_avg = []\n",
    "rate_list = []\n",
    "\n",
    "all_list = [auc_list,acc_best0,acc_best1,acc_best_ov,acc_best_avg,optimal_th_list]\n",
    "\n",
    "for sub in subs:\n",
    "    PATH = os.path.join(model_root,sub,'transient', 'model_checkpoint.pth')\n",
    "    EMA_PATH = os.path.join(model_root,sub,'transient_ema', 'model_checkpoint.pth')\n",
    "    model = load_model(PATH)\n",
    "    ema_model = load_model(EMA_PATH)\n",
    "\n",
    "    add_str=sub.split('_')[1]\n",
    "\n",
    "    rst_list = predict_path(model, test_rst_pre, add_str+'base')\n",
    "    ema_rst_list = predict_path(ema_model, test_rst_pre, add_str+'ema')\n",
    "    \n",
    "    rate_list.append(add_str[1:])\n",
    "    \n",
    "    if rst_list[0] > ema_rst_list[0]:\n",
    "        for ii,item in enumerate(all_list):\n",
    "            item.append(rst_list[ii])\n",
    "    else:\n",
    "        for ii,item in enumerate(all_list):\n",
    "            item.append(ema_rst_list[ii])\n",
    "\n",
    "df = pd.DataFrame({'rate':rate_list, 'auc':auc_list, 'best_acc0':acc_best0, 'best_acc1':acc_best1, \n",
    "                   'best_acc_ov':acc_best_ov, 'best_acc_avg':acc_best_avg, 'best_th':optimal_th_list})\n",
    "df.to_csv(os.path.join(model_root,'result.csv'), encoding='utf-8', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69656dea",
   "metadata": {},
   "source": [
    "## Bleeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c62d7fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T02:12:42.259297Z",
     "start_time": "2023-06-13T02:12:42.241282Z"
    }
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "test_dir = r\"~\\data\\imgs_feature_extraction_deep_learning\\imgs_bleeding\\test\"\n",
    "\n",
    "\n",
    "test_txt_path = os.path.join(test_dir, 'test.txt')\n",
    "test_file_list_0 = []\n",
    "test_file_list_1 = []\n",
    "dirnames = fetch_all_files(test_dir, file_exts = [\".jpg\", '.jpeg'])\n",
    "for i in range(len(dirnames)):\n",
    "    if os.path.split(dirnames[i])[0][-1] == '0':\n",
    "        test_file_list_0.append(dirnames[i])\n",
    "    elif os.path.split(dirnames[i])[0][-1] == '1':\n",
    "        test_file_list_1.append(dirnames[i])\n",
    "        \n",
    "with open(test_txt_path, 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(test_file_list_0)):\n",
    "        f.write(test_file_list_0[i]+',0'+'\\n')\n",
    "    for i in range(len(test_file_list_1)):\n",
    "        f.write(test_file_list_1[i]+',1'+'\\n')\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "channel_stats = dict(mean=[0.1713, 0.1608, 0.1504],std=[0.255, 0.25, 0.2458])\n",
    "test_dataset = test_txt_path\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "test_rst_pre = 'model_path'\n",
    "model_root = test_rst_pre\n",
    "#########################################################################################################################\n",
    "\n",
    "\n",
    "subs = [f for f in os.listdir(model_root) if os.path.isdir(os.path.join(model_root, f))] \n",
    "subs = natsorted(subs)\n",
    "\n",
    "eval_transformation = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**channel_stats)\n",
    "    ])\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "        TextFileDataset(test_dataset, eval_transformation),\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=2 * 2,\n",
    "        pin_memory=True,\n",
    "        drop_last=False)\n",
    "\n",
    "\n",
    "auc_list = []\n",
    "optimal_th_list = []\n",
    "acc_best0 = []\n",
    "acc_best1 = []\n",
    "acc_best_ov = []\n",
    "acc_best_avg = []\n",
    "rate_list = []\n",
    "\n",
    "all_list = [auc_list,acc_best0,acc_best1,acc_best_ov,acc_best_avg,optimal_th_list]\n",
    "\n",
    "for sub in subs:\n",
    "    PATH = os.path.join(model_root,sub,'transient', 'model_checkpoint.pth')\n",
    "    EMA_PATH = os.path.join(model_root,sub,'transient_ema', 'model_checkpoint.pth')\n",
    "    model = load_model(PATH)\n",
    "    ema_model = load_model(EMA_PATH)\n",
    "\n",
    "    add_str=sub.split('_')[1]\n",
    "\n",
    "    rst_list = predict_path(model, test_rst_pre, add_str+'base')\n",
    "    ema_rst_list = predict_path(ema_model, test_rst_pre, add_str+'ema')\n",
    "    \n",
    "    rate_list.append(add_str[1:])\n",
    "    \n",
    "    if rst_list[0] > ema_rst_list[0]:\n",
    "        for ii,item in enumerate(all_list):\n",
    "            item.append(rst_list[ii])\n",
    "    else:\n",
    "        for ii,item in enumerate(all_list):\n",
    "            item.append(ema_rst_list[ii])\n",
    "\n",
    "df = pd.DataFrame({'rate':rate_list, 'auc':auc_list, 'best_acc0':acc_best0, 'best_acc1':acc_best1, \n",
    "                   'best_acc_ov':acc_best_ov, 'best_acc_avg':acc_best_avg, 'best_th':optimal_th_list})\n",
    "df.to_csv(os.path.join(model_root,'result.csv'), encoding='utf-8', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2441a80",
   "metadata": {},
   "source": [
    "## Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea7af2ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T02:12:58.125055Z",
     "start_time": "2023-06-13T02:12:58.103958Z"
    }
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "test_dir = r\"~\\data\\imgs_feature_extraction_deep_learning\\imgs_tone\\test\"\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    fc_inputs = model.fc.in_features\n",
    "    model.fc = nn.Linear(fc_inputs, 3)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "def show_matrix(y_pred, y_true, classes_count, out_put_dir, fig_size=4, dpi=110, savefig=True):\n",
    "\n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(fig_size, fig_size), dpi=dpi)\n",
    "    classes = [str(x) for x in range(classes_count)]\n",
    "    \n",
    "    plot_confusion_matrix(cnf_matrix, classes=classes,\n",
    "                               title='Confusion matrix')\n",
    "\n",
    "    rand = random.randint(1, 1000)\n",
    "    file_name = time.strftime(\"%Y-%m-%d-%H-%M_\", time.localtime()) + str(rand) + \".jpg\"\n",
    "    file_path = os.path.abspath(out_put_dir + '/' + file_name)\n",
    "    if savefig:\n",
    "        plt.savefig(file_path)\n",
    "    err = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        if y_pred[i] != y_true[i]:\n",
    "            err += 1\n",
    "\n",
    "    overall_acc = 1 - err * 1.0 / len(y_pred)\n",
    "    acc_list = []\n",
    "    for i in range(cnf_matrix.shape[0]):\n",
    "        acc = 100 * cnf_matrix[i, i] / np.sum(cnf_matrix[i, :])\n",
    "        print('%02d acc: %.2f%%' % (i, acc))\n",
    "        acc_list.append(acc)\n",
    "    print('overall acc: %.2f%%, avg acc: %.2f%%' % (100 * overall_acc, np.mean(acc_list)))\n",
    "    acc_0 = format(acc_list[0],'.1f')\n",
    "    acc_1 = format(acc_list[1],'.1f')\n",
    "    acc_2 = format(acc_list[2],'.1f')\n",
    "    overall = format(100 * overall_acc,'.1f')\n",
    "    avg = format(np.mean(acc_list),'.1f')\n",
    "    result_list = [acc_0,acc_1,acc_2,overall,avg]\n",
    "    return result_list\n",
    "    \n",
    "def predict_path(model, to_chk_path, add_t=''):\n",
    "    model.eval() \n",
    "    \n",
    "    total_pred = []\n",
    "    total_true = np.array([])\n",
    "    total_zxd = []\n",
    "    \n",
    "    to_check_path_result = to_chk_path + '_result_%s' % add_t +'_'+ time.strftime(\"%y%m%d_%H%M%S\", time.localtime(time.time()))      \n",
    "    if not os.path.exists(to_check_path_result):\n",
    "            os.makedirs(to_check_path_result)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in eval_loader:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda(non_blocking=True)\n",
    "            score = model(X)\n",
    "            \n",
    "            _, prediction = torch.max(score, 1)\n",
    "            percentage = torch.nn.functional.softmax(score, dim=1) * 100\n",
    "            percentage_list = percentage.cpu().detach().numpy().tolist()\n",
    "            pred_cls = [item.index(max(item)) for item in percentage_list]\n",
    "            \n",
    "            cls1_zxd = [item[1] for item in percentage_list]\n",
    "            total_pred.extend(pred_cls)\n",
    "            total_true = np.concatenate((total_true, y.data.cpu()))\n",
    "            total_zxd.extend(cls1_zxd)\n",
    "    result_list = show_matrix(total_pred, total_true.tolist(), 3, to_check_path_result)\n",
    "    return result_list\n",
    "\n",
    "test_txt_path = os.path.join(test_dir, 'test.txt')\n",
    "test_file_list_0 = []\n",
    "test_file_list_1 = []\n",
    "test_file_list_2 = []\n",
    "dirnames = fetch_all_files(test_dir, file_exts = [\".jpg\", '.jpeg'])\n",
    "for i in range(len(dirnames)):\n",
    "    if os.path.split(dirnames[i])[0][-1] == '0':\n",
    "        test_file_list_0.append(dirnames[i])\n",
    "    elif os.path.split(dirnames[i])[0][-1] == '1':\n",
    "        test_file_list_1.append(dirnames[i])\n",
    "    elif os.path.split(dirnames[i])[0][-1] == '2':\n",
    "        test_file_list_2.append(dirnames[i])\n",
    "        \n",
    "with open(test_txt_path, 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(test_file_list_0)):\n",
    "        f.write(test_file_list_0[i]+',0'+'\\n')\n",
    "    for i in range(len(test_file_list_1)):\n",
    "        f.write(test_file_list_1[i]+',1'+'\\n')\n",
    "    for i in range(len(test_file_list_2)):\n",
    "        f.write(test_file_list_2[i]+',2'+'\\n')\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "channel_stats = dict(mean=[0.2018, 0.1929, 0.1886],std=[0.2741, 0.272, 0.2716])\n",
    "test_dataset = test_txt_path\n",
    "\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "test_rst_pre = 'model_path'\n",
    "model_root = test_rst_pre\n",
    "#########################################################################################################################\n",
    "\n",
    "\n",
    "subs = [f for f in os.listdir(model_root) if os.path.isdir(os.path.join(model_root, f))] \n",
    "subs = natsorted(subs)\n",
    "\n",
    "eval_transformation = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**channel_stats)\n",
    "    ])\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "        TextFileDataset(test_dataset, eval_transformation),\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=2 * 2, \n",
    "        pin_memory=True,\n",
    "        drop_last=False)\n",
    "\n",
    "\n",
    "acc_best0_base = []\n",
    "acc_best1_base = []\n",
    "acc_best2_base = []\n",
    "acc_best_ov_base = []\n",
    "acc_best_avg_base = []\n",
    "\n",
    "acc_best0_ema = []\n",
    "acc_best1_ema = []\n",
    "acc_best2_ema = []\n",
    "acc_best_ov_ema = []\n",
    "acc_best_avg_ema = []\n",
    "\n",
    "for sub in subs:\n",
    "    print(sub)\n",
    "    PATH = os.path.join(model_root,sub,'transient', 'model_checkpoint.pth')\n",
    "    EMA_PATH = os.path.join(model_root,sub,'transient_ema', 'model_checkpoint.pth')\n",
    "    model = load_model(PATH)\n",
    "    ema_model = load_model(EMA_PATH)\n",
    "\n",
    "    add_str=sub.split('_')[1]\n",
    "    result_list_base = predict_path(model, test_rst_pre, add_str+'base')\n",
    "    acc_best0_base.append(result_list_base[0])\n",
    "    acc_best1_base.append(result_list_base[1])\n",
    "    acc_best2_base.append(result_list_base[2])\n",
    "    acc_best_ov_base.append(result_list_base[3])\n",
    "    acc_best_avg_base.append(result_list_base[4])\n",
    "    print('base',result_list_base)\n",
    "    \n",
    "    result_list_ema = predict_path(ema_model, test_rst_pre, add_str+'ema')\n",
    "    print('ema',result_list_ema)\n",
    "    acc_best0_ema.append(result_list_ema[0])\n",
    "    acc_best1_ema.append(result_list_ema[1])\n",
    "    acc_best2_ema.append(result_list_ema[2])\n",
    "    acc_best_ov_ema.append(result_list_ema[3])\n",
    "    acc_best_avg_ema.append(result_list_ema[4])\n",
    "    \n",
    "    \n",
    "rate_list = [10,20,30,40,50,60,70,80,90]    \n",
    "df_base = pd.DataFrame({'rate':rate_list, 'best_acc0':acc_best0_base, 'best_acc1':acc_best1_base, 'best_acc2':acc_best2_base, \n",
    "                   'best_acc_ov':acc_best_ov_base, 'best_acc_avg':acc_best_avg_base})\n",
    "df_base.to_csv(os.path.join(model_root,'result_base.csv'), encoding='utf-8', index=False, sep=',')  \n",
    "\n",
    "df_ema = pd.DataFrame({'rate':rate_list, 'best_acc0':acc_best0_ema, 'best_acc1':acc_best1_ema, 'best_acc2':acc_best2_ema, \n",
    "                   'best_acc_ov':acc_best_ov_ema, 'best_acc_avg':acc_best_avg_ema})\n",
    "df_ema.to_csv(os.path.join(model_root,'result_ema.csv'), encoding='utf-8', index=False, sep=',') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3887ef9",
   "metadata": {},
   "source": [
    "## Elevated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e367ccd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T02:13:15.467610Z",
     "start_time": "2023-06-13T02:13:15.448624Z"
    }
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "test_dir = r\"~\\data\\imgs_feature_extraction_deep_learning\\imgs_elevated\\test\"\n",
    "\n",
    "\n",
    "test_txt_path = os.path.join(test_dir, 'test.txt')\n",
    "test_file_list_0 = []\n",
    "test_file_list_1 = []\n",
    "dirnames = fetch_all_files(test_dir, file_exts = [\".jpg\", '.jpeg'])\n",
    "for i in range(len(dirnames)):\n",
    "    if os.path.split(dirnames[i])[0][-1] == '0':\n",
    "        test_file_list_0.append(dirnames[i])\n",
    "    elif os.path.split(dirnames[i])[0][-1] == '1':\n",
    "        test_file_list_1.append(dirnames[i])\n",
    "        \n",
    "with open(test_txt_path, 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(test_file_list_0)):\n",
    "        f.write(test_file_list_0[i]+',0'+'\\n')\n",
    "    for i in range(len(test_file_list_1)):\n",
    "        f.write(test_file_list_1[i]+',1'+'\\n')\n",
    "f.close()\n",
    "\n",
    "\n",
    "channel_stats = dict(mean=[0.2167, 0.2076, 0.202],std=[0.2811, 0.2788, 0.2781])\n",
    "test_dataset = test_txt_path\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "test_rst_pre = 'model_path'\n",
    "model_root = test_rst_pre\n",
    "#########################################################################################################################\n",
    "\n",
    "\n",
    "subs = [f for f in os.listdir(model_root) if os.path.isdir(os.path.join(model_root, f))] \n",
    "subs = natsorted(subs)\n",
    "\n",
    "eval_transformation = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**channel_stats)\n",
    "    ])\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "        TextFileDataset(test_dataset, eval_transformation),\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=2 * 2,\n",
    "        pin_memory=True,\n",
    "        drop_last=False)\n",
    "\n",
    "\n",
    "auc_list = []\n",
    "optimal_th_list = []\n",
    "acc_best0 = []\n",
    "acc_best1 = []\n",
    "acc_best_ov = []\n",
    "acc_best_avg = []\n",
    "rate_list = []\n",
    "\n",
    "all_list = [auc_list,acc_best0,acc_best1,acc_best_ov,acc_best_avg,optimal_th_list]\n",
    "\n",
    "for sub in subs:\n",
    "    PATH = os.path.join(model_root,sub,'transient', 'model_checkpoint.pth')\n",
    "    EMA_PATH = os.path.join(model_root,sub,'transient_ema', 'model_checkpoint.pth')\n",
    "    model = load_model(PATH)\n",
    "    ema_model = load_model(EMA_PATH)\n",
    "\n",
    "    add_str=sub.split('_')[1]\n",
    "\n",
    "    rst_list = predict_path(model, test_rst_pre, add_str+'base')\n",
    "    ema_rst_list = predict_path(ema_model, test_rst_pre, add_str+'ema')\n",
    "    \n",
    "    rate_list.append(add_str[1:])\n",
    "    \n",
    "    if rst_list[0] > ema_rst_list[0]:\n",
    "        for ii,item in enumerate(all_list):\n",
    "            item.append(rst_list[ii])\n",
    "    else:\n",
    "        for ii,item in enumerate(all_list):\n",
    "            item.append(ema_rst_list[ii])\n",
    "\n",
    "df = pd.DataFrame({'rate':rate_list, 'auc':auc_list, 'best_acc0':acc_best0, 'best_acc1':acc_best1, \n",
    "                   'best_acc_ov':acc_best_ov, 'best_acc_avg':acc_best_avg, 'best_th':optimal_th_list})\n",
    "df.to_csv(os.path.join(model_root,'result.csv'), encoding='utf-8', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ee714",
   "metadata": {},
   "source": [
    "### Depressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86e1d6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T02:11:42.323719Z",
     "start_time": "2023-06-13T02:11:42.303684Z"
    }
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "test_dir = r\"~\\data\\imgs_feature_extraction_deep_learning\\imgs_depressed\\test\"\n",
    "\n",
    "test_txt_path = os.path.join(test_dir, 'test.txt')\n",
    "test_file_list_0 = []\n",
    "test_file_list_1 = []\n",
    "dirnames = fetch_all_files(test_dir, file_exts = [\".jpg\", '.jpeg'])\n",
    "for i in range(len(dirnames)):\n",
    "    if os.path.split(dirnames[i])[0][-1] == '0':\n",
    "        test_file_list_0.append(dirnames[i])\n",
    "    elif os.path.split(dirnames[i])[0][-1] == '1':\n",
    "        test_file_list_1.append(dirnames[i])\n",
    "        \n",
    "with open(test_txt_path, 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(test_file_list_0)):\n",
    "        f.write(test_file_list_0[i]+',0'+'\\n')\n",
    "    for i in range(len(test_file_list_1)):\n",
    "        f.write(test_file_list_1[i]+',1'+'\\n')\n",
    "f.close()\n",
    "\n",
    "\n",
    "channel_stats = dict(mean=[0.1988, 0.193, 0.1845],std=[0.2712, 0.2702, 0.2663])\n",
    "test_dataset = test_txt_path\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "test_rst_pre = 'model_path'\n",
    "model_root = test_rst_pre\n",
    "#########################################################################################################################\n",
    "\n",
    "\n",
    "subs = [f for f in os.listdir(model_root) if os.path.isdir(os.path.join(model_root, f))] \n",
    "subs = natsorted(subs)\n",
    "\n",
    "eval_transformation = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**channel_stats)\n",
    "    ])\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "        TextFileDataset(test_dataset, eval_transformation),\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=2 * 2,\n",
    "        pin_memory=True,\n",
    "        drop_last=False)\n",
    "\n",
    "\n",
    "auc_list = []\n",
    "optimal_th_list = []\n",
    "acc_best0 = []\n",
    "acc_best1 = []\n",
    "acc_best_ov = []\n",
    "acc_best_avg = []\n",
    "rate_list = []\n",
    "\n",
    "all_list = [auc_list,acc_best0,acc_best1,acc_best_ov,acc_best_avg,optimal_th_list]\n",
    "\n",
    "for sub in subs:\n",
    "    PATH = os.path.join(model_root,sub,'transient', 'model_checkpoint.pth')\n",
    "    EMA_PATH = os.path.join(model_root,sub,'transient_ema', 'model_checkpoint.pth')\n",
    "    model = load_model(PATH)\n",
    "    ema_model = load_model(EMA_PATH)\n",
    "\n",
    "    add_str=sub.split('_')[1]\n",
    "\n",
    "    rst_list = predict_path(model, test_rst_pre, add_str+'base')\n",
    "    ema_rst_list = predict_path(ema_model, test_rst_pre, add_str+'ema')\n",
    "    \n",
    "    rate_list.append(add_str[1:])\n",
    "    \n",
    "    if rst_list[0] > ema_rst_list[0]:\n",
    "        for ii,item in enumerate(all_list):\n",
    "            item.append(rst_list[ii])\n",
    "    else:\n",
    "        for ii,item in enumerate(all_list):\n",
    "            item.append(ema_rst_list[ii])\n",
    "\n",
    "df = pd.DataFrame({'rate':rate_list, 'auc':auc_list, 'best_acc0':acc_best0, 'best_acc1':acc_best1, \n",
    "                   'best_acc_ov':acc_best_ov, 'best_acc_avg':acc_best_avg, 'best_th':optimal_th_list})\n",
    "df.to_csv(os.path.join(model_root,'result.csv'), encoding='utf-8', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f9227",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
